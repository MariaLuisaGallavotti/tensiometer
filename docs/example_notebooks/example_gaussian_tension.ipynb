{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agreement and disagreement between Gaussian posterior distributions\n",
    "\n",
    "Marco Raveri (<marco.raveri@unige.it>)\n",
    "\n",
    "This notebook shows an end to end calculation of the agreement and disagreement between two experiments with different methods, in an idealized case and in a realistic case involving two cosmological experiments.\n",
    "\n",
    "This implements the methods discussed in [Raveri and Hu (2018), arXiv:1806.04649](https://arxiv.org/abs/1806.04649)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show plots inline, and load main getdist plot module and samples class\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "# import libraries:\n",
    "import sys, os\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'  # needed for tensorflow KERAS compatibility\n",
    "os.environ['DISPLAY'] = 'inline'  # hack to get getdist working\n",
    "sys.path.insert(0,os.path.realpath(os.path.join(os.getcwd(),'../..')))\n",
    "from getdist import plots, MCSamples\n",
    "from getdist.gaussian_mixtures import GaussianND\n",
    "import getdist\n",
    "getdist.chains.print_load_details = False\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython\n",
    "import numpy as np\n",
    "# tensiometer imports:\n",
    "from tensiometer import utilities\n",
    "from tensiometer import gaussian_tension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy example: \n",
    "\n",
    "For a toy example we consider two 2D Gaussian distributions. \n",
    "One has correlation between the two parameters while the other does not.\n",
    "The two Gaussian distributions are displaced a little to result in a mild tension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the two toy chains and the joint chain:\n",
    "ndim = 2 # number of dimensions\n",
    "nsamp = 10000 # number of samples\n",
    "nchains = 8 # number of chains\n",
    "# helper function to compute the likelihood:\n",
    "def helper_gaussian_log_like( samps, mean, invcov ):\n",
    "    _diff = samps-mean\n",
    "    return (_diff.dot(invcov)*_diff).sum(axis=1)\n",
    "# first data set:\n",
    "mean_1 = np.array([0., 0.])\n",
    "cov_1 = np.array([[1, -0.9], [-0.9, 1]])\n",
    "samps_1 = np.random.multivariate_normal(mean_1, cov_1, size=nchains*nsamp)\n",
    "log_like_1 = helper_gaussian_log_like(samps_1, mean_1, np.linalg.inv(cov_1))\n",
    "# second data set:\n",
    "mean_2 = np.array([1., 1.])\n",
    "cov_2 = np.array([[.09, 0.], [0., .09]])\n",
    "samps_2 = np.random.multivariate_normal( mean_2, cov_2, size=nchains*nsamp)\n",
    "log_like_2 = helper_gaussian_log_like(samps_2, mean_2, np.linalg.inv(cov_2))\n",
    "# joint data set:\n",
    "cov_12 = np.linalg.inv(np.linalg.inv(cov_1) + np.linalg.inv(cov_2))\n",
    "mean_12 = np.dot(cov_12, np.dot(np.linalg.inv(cov_1), mean_1) + np.dot(np.linalg.inv(cov_2), mean_2))\n",
    "samps_12 = np.random.multivariate_normal(mean_12, cov_12, size=nchains*nsamp)\n",
    "log_like_12 = helper_gaussian_log_like(samps_12, mean_1, np.linalg.inv(cov_1)) \\\n",
    "            + helper_gaussian_log_like(samps_12, mean_2, np.linalg.inv(cov_2))\n",
    "# initialize the parameter names:\n",
    "names = [\"p%s\"%i for i in range(ndim)]\n",
    "labels = [\"p_%s\"%i for i in range(ndim)]\n",
    "# initialize the GetDist chains:\n",
    "chain_1 = MCSamples(samples=samps_1, loglikes=log_like_1, names=names, labels=labels, label='first')\n",
    "chain_2 = MCSamples(samples=samps_2, loglikes=log_like_2, names=names, labels=labels, label='second')\n",
    "chain_12 = MCSamples(samples=samps_12, loglikes=log_like_12, names=names, labels=labels, label='joint')\n",
    "# separate the chains so that we can have rough convergence estimates:\n",
    "for chain in [chain_1, chain_2, chain_12]:\n",
    "    chain.chain_offsets = [i*nsamp for i in range(nchains+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the distributions to make sure we are getting a good example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plots.get_subplot_plotter()\n",
    "g.triangle_plot([chain_1, chain_2, chain_12], filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly if we were to look only at the marginalized posteriors of the two distributions we would probably not guess that the two chains are in tension with each other.\n",
    "We next proceed to the calculation of the statistical significance of this tension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can proceed to compute the statistical significance of the difference in parameters with other estimators, as discussed in [Raveri and Hu (2018), arXiv:1806.04649](https://arxiv.org/abs/1806.04649).\n",
    "These rely on the Gaussian approximation (though some of them are defined in a way that mitigates possible non-Gaussianities).\n",
    "\n",
    "### Parameter shifts in standard form\n",
    "\n",
    "We start by calculating the difference between the means of the two distributions, $\\theta_1$ and $\\theta_2$.\n",
    "For optimal weighting of these difference we use the covariance of the $\\theta_1-\\theta_2$ shift:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{C}_{\\rm DM} =\n",
    "\\end{align}\n",
    "\n",
    "note that this is significantly more complicated than what one might be used to, due to the fact that we are accounting for the presence of a possibly informative prior.\n",
    "Then the optimal estimator of the significance of the shift is:\n",
    "\n",
    "\\begin{align}\n",
    "Q_{\\rm DM} \\equiv (\\theta_1-\\theta_2)^T \\, \\mathcal{C}_{\\rm DM}^{-1} \\, (\\theta_1-\\theta_2) \\sim \\chi^2\\left[ \\mathrm{rank}\\, \\mathcal{C}_{\\rm DM}^{-1}\\right]\n",
    "\\end{align}\n",
    "\n",
    "After calculating the value of $Q_{\\rm DM}$ and its degrees of freedom we can calculate the probability to exceed the observed value (which is the statistical significance of the tension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Q_DM and its dofs:\n",
    "Q_DM, Q_DM_dofs = gaussian_tension.Q_DM( chain_1, chain_2 )\n",
    "# calculate probability to exceed:\n",
    "Q_DM_P = scipy.stats.chi2.cdf(Q_DM, Q_DM_dofs)\n",
    "# print everything:\n",
    "print(f'Q_DM = {Q_DM:.2f}, dofs = {Q_DM_dofs:2}, P = {Q_DM_P:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilities are usually terrible numbers. The reason beeing that we care about probabilities on a log scale.\n",
    "We can convert probability values to effective number of sigmas, matching the probability level of the result to those of a Gaussian distribution:\n",
    "\n",
    "\\begin{align}\n",
    "n_{\\sigma} = \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to effective number of sigmas:\n",
    "Q_DM_nsigma = utilities.from_confidence_to_sigma(Q_DM_P)\n",
    "# print everything:\n",
    "print(f'nsigma = {Q_DM_nsigma:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter shifts in update form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start calculations:\n",
    "Q_DM, Q_DM_dofs = gaussian_tension.Q_DM( chain_1, chain_2 )\n",
    "Q_UDM, Q_UDM_dofs =gaussian_tension.Q_UDM( chain_1, chain_12, lower_cutoff=1. )\n",
    "Q_DM_P = scipy.stats.chi2.cdf(Q_DM, Q_DM_dofs)\n",
    "Q_UDM_P = scipy.stats.chi2.cdf(Q_UDM, Q_UDM_dofs)\n",
    "Q_DM_nsigma = utilities.from_confidence_to_sigma(Q_DM_P)\n",
    "Q_UDM_nsigma = utilities.from_confidence_to_sigma(Q_UDM_P)\n",
    "print(f'Q_DM = {Q_DM:.2f}, dofs = {Q_DM_dofs:2}, P = {Q_DM_P:.5f}, nsigma = {Q_DM_nsigma:.3f}')\n",
    "print(f'Q_UDM = {Q_UDM:.2f}, dofs = {Q_UDM_dofs:2}, P = {Q_UDM_P:.5f}, nsigma = {Q_UDM_nsigma:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That we see match very well, to a fraction of a sigma.\n",
    "\n",
    "We can now test the behavior of Q_UDM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that Q_UDM is symmetric in the Gaussian case:\n",
    "print('Symmetric Q_UDM = %.3f, dofs = %2i' % gaussian_tension.Q_UDM(chain_2, chain_12, lower_cutoff=1.))\n",
    "# find the optimal cutoff by providing the second chain:\n",
    "print('Q_UDM optimal cutoff = %.3f' % gaussian_tension.Q_UDM_get_cutoff(chain_1, chain_2, chain_12)[0])\n",
    "# get the KL spectrum:\n",
    "print('Q_UDM KL spectrum:', gaussian_tension.Q_UDM_KL_components(chain_1, chain_12)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the estimator is symmetric, none of the parameters we are considering is prior limited and the optimal cutoff just leave all directions come through.\n",
    "We can proceed to compute the chi squared based version of these estimators.\n",
    "This consists in the statistics of Goodness of fit loss (Q_DMAP) which gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_DMAP, Q_DMAP_dofs = gaussian_tension.Q_DMAP(chain_1, chain_2, chain_12, feedback=0 )\n",
    "Q_DMAP_P = scipy.stats.chi2.cdf(Q_DMAP, Q_DMAP_dofs)\n",
    "Q_DMAP_nsigma = utilities.from_confidence_to_sigma(Q_DMAP_P)\n",
    "print(f'Q_DMAP = {Q_DMAP:.2f}, dofs = {Q_DMAP_dofs:2}, P = {Q_DMAP_P:.5f}, nsigma = {Q_DMAP_nsigma:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is in very good agreement with the previous results, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A realistic example, Planck 2018 and DES Y1:\n",
    "\n",
    "We now turn to a more realistic example that involves a chains from two different (real) experiments.\n",
    "We use the LCDM parameter chains for the results of Planck 2018 (https://arxiv.org/abs/1807.06209) and the results for the Dark Energy Survey (DES)\n",
    "first year of data (https://arxiv.org/abs/1708.01530).\n",
    "\n",
    "Notice that we have removed many parameters from the chains since they were irrelevant to the example.\n",
    "The chains are already fully polished. Burn in has been removed and the samples have been thinned.\n",
    "\n",
    "Notice that we have run a prior only chain to ensure that the modeling of the prior is as faithful as possible.\n",
    "In particular, in standard cosmological analyses, we have priors on derived parameters that would give non-trivial shapes to the parameters that are being sampled (see Appendix F in https://arxiv.org/abs/1806.04649)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the samples (remove no burn in since the example chains have already been cleaned):\n",
    "chains_dir = os.path.realpath(os.path.join(os.getcwd(), '../..', 'test_chains'))\n",
    "# the Planck 2018 TTTEEE chain:\n",
    "chain_1 = getdist.mcsamples.loadMCSamples(file_root=os.path.join(chains_dir, 'Planck18TTTEEE'), no_cache=True)\n",
    "# the DES Y1 3x2 chain:\n",
    "chain_2 = getdist.mcsamples.loadMCSamples(file_root=os.path.join(chains_dir, 'DES'), no_cache=True)\n",
    "# the joint chain:\n",
    "chain_12 = getdist.mcsamples.loadMCSamples(file_root=os.path.join(chains_dir, 'Planck18TTTEEE_DES'), no_cache=True)\n",
    "# the prior chain:\n",
    "prior_chain = getdist.mcsamples.loadMCSamples(file_root=os.path.join(chains_dir, 'prior'), no_cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first plot the chains to get some intuition of what is happening.\n",
    "We show only the parameters that have been run and that the two chains share.\n",
    "These are the only parameters that can contribute to a tension between the two experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_param_names = [name for name in chain_1.getParamNames().getRunningNames() \n",
    "                      if name in chain_2.getParamNames().getRunningNames()]\n",
    "g = plots.get_subplot_plotter()\n",
    "g.triangle_plot([chain_1, chain_2, chain_12], params=shared_param_names, filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this plot it is not obvious at all whether the two experiments agree or not. \n",
    "\n",
    "Moreover, while Planck might still look Gaussian, DES looks like it has a very non-Gaussian posterior.\n",
    "\n",
    "If we try another parameter combination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plots.get_subplot_plotter()\n",
    "g.triangle_plot([chain_2, chain_1, chain_12], params=['sigma8','omegam'], filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The situation looks very different. We have selected the two parameters that the weakest data set (DES) best constrains and in this plane the two data sets do not look in agreement.\n",
    "They look fairly similar to our toy example! Coincidence?\n",
    "\n",
    "The posteriors also look more Gaussian since we are selecting at the best constrained directions. \n",
    "This is a very common problem. When looking at some parameter that is partially informed by the prior the posterior might look non-Gaussian because of prior volume projections.\n",
    "\n",
    "We can compute the number of parameters that our data sets are truly constraining over the prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Neff_1 = gaussian_tension.get_Neff(chain_1, param_names=shared_param_names, prior_chain=prior_chain)\n",
    "Neff_2 = gaussian_tension.get_Neff(chain_2, param_names=shared_param_names, prior_chain=prior_chain)\n",
    "Neff_12 = gaussian_tension.get_Neff(chain_12, param_names=shared_param_names, prior_chain=prior_chain)\n",
    "print(f'Neff(Planck) = {Neff_1:.2f}, Neff(DES) = {Neff_2:.2f}, Neff(joint) = {Neff_12:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is no surprise that Planck is constraining all six cosmological parameters while DES is constraining three.\n",
    "In addition to the two parameters, that we have shown in the plot above, DES is constraining the angular scale of the BAO peak, as we can see by directly feeding the parameters to use in the calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Neff_2 = gaussian_tension.get_Neff(chain_2, param_names=['sigma8','omegam','theta_BAO_DES'], prior_chain=prior_chain)\n",
    "print(f'Neff(DES) = {Neff_2:.2f}')\n",
    "# the extra 0.5 parameter is a very weak constraint on n_s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since these are real data we can check the Goodness of their fit at maximum posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## in our case the DES data set has 457 data points and the Planck one has 2289\n",
    "# if we were to normalize the likelihood we need to subtract the normalization factor to get the chi2.\n",
    "Q_MAP_1 = gaussian_tension.Q_MAP(chain_1, num_data=2289, prior_chain=prior_chain)\n",
    "Q_MAP_2 = gaussian_tension.Q_MAP(chain_2, num_data=457, prior_chain=prior_chain)\n",
    "Q_MAP_12 = gaussian_tension.Q_MAP(chain_12, num_data=2289+457, prior_chain=prior_chain)\n",
    "# get the probability:\n",
    "Q_MAP_1_P = scipy.stats.chi2.cdf(*Q_MAP_1)\n",
    "Q_MAP_2_P = scipy.stats.chi2.cdf(*Q_MAP_2)\n",
    "Q_MAP_12_P = scipy.stats.chi2.cdf(*Q_MAP_12)\n",
    "# print results:\n",
    "print(f'Goodness of fit for Planck TTTEEE: Q_MAP = {Q_MAP_1[0]:.3f} with dofs {Q_MAP_1[1]:.3f}')\n",
    "print(f'Q_MAP probability = {Q_MAP_1_P:.5f}, n_sigma = {utilities.from_confidence_to_sigma(Q_MAP_1_P):.3f}\\n')\n",
    "print(f'Goodness of fit for DES: Q_MAP = {Q_MAP_2[0]:.3f} with dofs {Q_MAP_2[1]:.3f}')\n",
    "print(f'Q_MAP probability = {Q_MAP_2_P:.5f}, n_sigma = {utilities.from_confidence_to_sigma(Q_MAP_2_P):.3f}\\n')\n",
    "print(f'Goodness of fit for joint: Q_MAP = {Q_MAP_12[0]:.3f} with dofs {Q_MAP_12[1]:.3f}')\n",
    "print(f'Q_MAP probability = {Q_MAP_12_P:.5f}, n_sigma = {utilities.from_confidence_to_sigma(Q_MAP_12_P):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that the maximum posterior is a fairly good fit to the data.\n",
    "\n",
    "Then we check the reliability of the Gaussian approximation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the Gaussian approximation:\n",
    "gaussian_1 = gaussian_tension.gaussian_approximation(chain_1)\n",
    "gaussian_2 = gaussian_tension.gaussian_approximation(chain_2)\n",
    "# plot for comparison:\n",
    "g = plots.get_subplot_plotter()\n",
    "g.triangle_plot([chain_1, chain_2, gaussian_1, gaussian_2], params=['sigma8','omegam','theta_BAO_DES'], filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian estimators:\n",
    "\n",
    "We start by considering parameter shifts in standard form over the three constrained parameters and over the full parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with Q_DM:\n",
    "Q_DM = gaussian_tension.Q_DM(chain_1, chain_2, prior_chain=prior_chain, param_names=['sigma8', 'omegam','theta_BAO_DES'])\n",
    "Q_DM_full = gaussian_tension.Q_DM(chain_1, chain_2, prior_chain=prior_chain, param_names=shared_param_names)\n",
    "# compute probability:\n",
    "Q_DM_P = scipy.stats.chi2.cdf(*Q_DM)\n",
    "Q_DM_full_P = scipy.stats.chi2.cdf(*Q_DM_full)\n",
    "# print results:\n",
    "print(f'Using only sigma8 and Omegam: Q_DM = {Q_DM[0]:.3f} with dofs {Q_DM[1]:.3f}')\n",
    "print(f'Q_DM probability = {Q_DM_P:.5f}, n_sigma = {utilities.from_confidence_to_sigma(Q_DM_P):.3f}')\n",
    "print(f'Using all parameters: Q_DM = {Q_DM_full[0]:.3f} with dofs {Q_DM_full[1]:.3f}')\n",
    "print(f'Q_DM probability = {Q_DM_full_P:.5f}, n_sigma = {utilities.from_confidence_to_sigma(Q_DM_full_P):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the result is significantly different and different from what we would expect from the previous results. This happens because, in standard form, Gaussian parameter shifts are not mitigated against non-Gaussianities. Moreover, in the full parameter space the effect of the prior is hard to be removed. As we can see the full parameter space Q_DM is reporting five degrees of freedom which is wrong since we know that that cannot exceed the number of parameters measured by DES (3).\n",
    "\n",
    "Parameter shifts in update form, that we consider next, offer some mitigation against non-Gaussianities while being more effective at detecting data constrained parameter space directions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now Q_UDM:\n",
    "Q_UDM = gaussian_tension.Q_UDM(chain_1, chain_12, param_names=['sigma8', 'omegam','theta_BAO_DES'])\n",
    "Q_UDM_full = gaussian_tension.Q_UDM(chain_1, chain_12, param_names=shared_param_names)\n",
    "# compute probability:\n",
    "Q_UDM_P = scipy.stats.chi2.cdf(*Q_UDM)\n",
    "Q_UDM_full_P = scipy.stats.chi2.cdf(*Q_UDM_full)\n",
    "# print results:\n",
    "print(f'Using only sigma8 and Omegam: Q_UDM = {Q_UDM[0]:.3f} with dofs {Q_UDM[1]:.3f}')\n",
    "print(f'Q_UDM probability = {Q_UDM_P:.5f}, n_sigma = {utilities.from_confidence_to_sigma(Q_UDM_P):.3f}')\n",
    "print(f'Using all parameters: Q_UDM = {Q_UDM_full[0]:.3f} with dofs {Q_UDM_full[1]:.3f}')\n",
    "print(f'Q_UDM probability = {Q_UDM_full_P:.5f}, n_sigma = {utilities.from_confidence_to_sigma(Q_UDM_full_P):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see these two estimates agree very well with each other since the separation of data constrained directions is more stable. The results also agree very well with the exact result showing that non-Gaussianities are effectively mitigated by considering Planck as the base for the update.\n",
    "\n",
    "In this case it is important to select as the base for the update parameter shifts the most constraining (and most Gaussian) data set. If we were to use DES (which is non-Gaussian!) as a base for the update we would get a different result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now Q_UDM:\n",
    "Q_UDM = gaussian_tension.Q_UDM(chain_2, chain_12, param_names=['sigma8', 'omegam','theta_BAO_DES'])\n",
    "Q_UDM_full = gaussian_tension.Q_UDM(chain_2, chain_12, param_names=shared_param_names)\n",
    "# compute probability:\n",
    "Q_UDM_P = scipy.stats.chi2.cdf(*Q_UDM)\n",
    "Q_UDM_full_P = scipy.stats.chi2.cdf(*Q_UDM_full)\n",
    "# print results:\n",
    "print(f'Using only sigma8 and Omegam: Q_UDM = {Q_UDM[0]:.3f} with dofs {Q_UDM[1]:.3f}')\n",
    "print(f'Q_UDM probability = {Q_UDM_P:.5f}, n_sigma = {utilities.from_confidence_to_sigma(Q_UDM_P):.3f}')\n",
    "print(f'Using all parameters: Q_UDM = {Q_UDM_full[0]:.3f} with dofs {Q_UDM_full[1]:.3f}')\n",
    "print(f'Q_UDM probability = {Q_UDM_full_P:.5f}, n_sigma = {utilities.from_confidence_to_sigma(Q_UDM_full_P):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is wrong since it is heavily contaminated by non-Gaussianities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we compute Goodness of fit loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at last Q_DMAP:\n",
    "Q_DMAP = gaussian_tension.Q_DMAP(chain_1, chain_2, chain_12, prior_chain=prior_chain, param_names=shared_param_names)\n",
    "Q_DMAP_P = scipy.stats.chi2.cdf(Q_DMAP[0],Q_DMAP[1])\n",
    "print(f'Using all parameters: Q_DMAP = {Q_DMAP[0]:.3f} with dofs {Q_DMAP[1]:.3f}')\n",
    "print(f'Q_DMAP probability = {Q_DMAP_P:.5f}, n_sigma = {utilities.from_confidence_to_sigma(Q_DMAP_P):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that we see agrees with the correct result, to a fraction of a sigma.\n",
    "\n",
    "Possibly some of the discrepancy here is that the estimator is not fully mitigating non-Gaussianities.\n",
    "\n",
    "Overall we can conclude that a set of reliable estimators agree on a tension between our two input data sets that ranges between 2.8 and 3.2 sigma."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensiometer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
