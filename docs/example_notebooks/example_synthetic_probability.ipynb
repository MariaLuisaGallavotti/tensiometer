{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic models for posterior distributions\n",
    "\n",
    "Marco Raveri (<marco.raveri@unige.it>), Cyrille Doux (<doux@lpsc.in2p3.fr>), Shivam Pandey (<shivampcosmo@gmail.com>)\n",
    "\n",
    "In this notebook we show how to build normalizing flow syntetic models for posterior distributions, as in [Raveri, Doux and Pandey (2024), arXiv:XXXX.XXXX](https://arxiv.org/abs/XXXX.XXXXX)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show plots inline, and load main getdist plot module and samples class\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import libraries:\n",
    "import sys, os\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'  # needed for tensorflow KERAS compatibility\n",
    "os.environ['DISPLAY'] = 'inline'  # hack to get getdist working\n",
    "sys.path.insert(0,os.path.realpath(os.path.join(os.getcwd(),'../..')))\n",
    "from getdist import plots, MCSamples\n",
    "from getdist.gaussian_mixtures import GaussianND\n",
    "import getdist\n",
    "getdist.chains.print_load_details = False\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# tensorflow imports:\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "# import the tensiometer tools that we need:\n",
    "import tensiometer\n",
    "from tensiometer import utilities\n",
    "from tensiometer.synthetic_probability import synthetic_probability as sp\n",
    "\n",
    "# getdist settings to ensure consistency of plots:\n",
    "getdist_settings = {'ignore_rows': 0.0, \n",
    "                    'smooth_scale_2D': 0.3,\n",
    "                    'smooth_scale_1D': 0.3,\n",
    "                    }    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by building a random Gaussian mixture that we are going to use for tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameters of the problem:\n",
    "dim = 6\n",
    "num_gaussians = 3\n",
    "num_samples = 10000\n",
    "\n",
    "# we seed the random number generator to get reproducible results:\n",
    "seed = 100\n",
    "np.random.seed(seed)\n",
    "# we define the range for the means and covariances:\n",
    "mean_range = (-0.5, 0.5)\n",
    "cov_scale = 0.4**2\n",
    "# means and covs:\n",
    "means = np.random.uniform(mean_range[0], mean_range[1], num_gaussians*dim).reshape(num_gaussians, dim)\n",
    "weights = np.random.rand(num_gaussians)\n",
    "weights = weights / np.sum(weights)\n",
    "covs = [cov_scale*utilities.vector_to_PDM(np.random.rand(int(dim*(dim+1)/2))) for _ in range(num_gaussians)]\n",
    "\n",
    "# cast to required precision:\n",
    "means = means.astype(np.float32)\n",
    "weights = weights.astype(np.float32)\n",
    "covs = [cov.astype(np.float32) for cov in covs]\n",
    "\n",
    "# initialize distribution:\n",
    "distribution = tfp.distributions.Mixture(\n",
    "    cat=tfp.distributions.Categorical(probs=weights),\n",
    "    components=[\n",
    "        tfp.distributions.MultivariateNormalTriL(loc=_m, scale_tril=tf.linalg.cholesky(_c))\n",
    "        for _m, _c in zip(means, covs)\n",
    "    ], name='Mixture')\n",
    "\n",
    "# sample the distribution:\n",
    "samples = distribution.sample(num_samples).numpy()\n",
    "# calculate log posteriors:\n",
    "logP = distribution.log_prob(samples).numpy()\n",
    "\n",
    "# create MCSamples from the samples:\n",
    "chain = MCSamples(samples=samples, \n",
    "                    settings=getdist_settings,\n",
    "                    loglikes=-logP,\n",
    "                    name_tag='Mixture',\n",
    "                    )\n",
    "\n",
    "# we make a sanity check plot:\n",
    "g = plots.get_subplot_plotter()\n",
    "g.triangle_plot(chain, filled=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base example: \n",
    "\n",
    "We train a normalizing flow on samples of a given distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize and train the normalizing flow on samples of the distribution we have just defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "          'feedback': 2,\n",
    "          'plot_every': 1000,\n",
    "          'pop_size': 1,\n",
    "          #'cache_dir': 'test',  # set this to a directory to cache the results\n",
    "          #'root_name': 'test',  # sets the name of the flow for the cache files\n",
    "        }\n",
    "\n",
    "flow = sp.flow_from_chain(chain,  # parameter difference chain\n",
    "                          **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can plot training summaries to make sure training went smoothly:\n",
    "flow.training_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and we can print the training summary:\n",
    "flow.print_training_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can triangle plot the flow to see how well it has learned the target distribution:\n",
    "g = plots.get_subplot_plotter()\n",
    "g.triangle_plot([chain, flow.MCSamples(20000)], \n",
    "                params=flow.param_names,\n",
    "                filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this looks nice but not perfect, let's train for longer:\n",
    "flow.feedback = 1\n",
    "flow.train(epochs=300, verbose=-1);  # verbose = -1 uses tqdm progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can plot training summaries to make sure training went smoothly:\n",
    "flow.training_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you train for long enough you should start seeing the learning rate adapting to the non-improving (noisy)  loss function.\n",
    "\n",
    "This means that the flow is learning finer and finer features and a good indication that training is converging. \n",
    "If you push it further, at some point, the flow will start overfitting and training will stop.\n",
    "\n",
    "Now let's look at how the marginal distributions look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can triangle plot the flow to see how well it has learned the target distribution:\n",
    "g = plots.get_subplot_plotter()\n",
    "g.triangle_plot([chain, \n",
    "                 flow.MCSamples(20000)  # this flow method returns a MCSamples object\n",
    "                 ], \n",
    "                params=flow.param_names,\n",
    "                filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is now much better!\n",
    "\n",
    "We can use the trained flow to perform several operations. For example let's compute log-likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = flow.MCSamples(20000)\n",
    "logP = flow.log_probability(flow.cast(samples.samples)).numpy()\n",
    "samples.addDerived(logP, name='logP', label='\\\\log P')\n",
    "samples.updateBaseStatistics();\n",
    "\n",
    "# now let's plot everything:\n",
    "g = plots.get_subplot_plotter()\n",
    "g.triangle_plot([samples, chain], \n",
    "                plot_3d_with_param='logP',\n",
    "                filled=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can appreciate here a beautiful display of a projection effect. The marginal distribution of $p_5$ is peaked at a positive value while the logP plot clearly shows that the peak of the full distribution is the negative one.\n",
    "\n",
    "If you are interested in understanding systematically these types of effect, check the corresponding tensiometer tutorial!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average flow example:\n",
    "\n",
    "A more advanced flow model consists in training several flows and using a weighted mixture normalizing flow model.\n",
    "\n",
    "This flow model improves the variance of the flow in regions that are scarse with samples (as different flow models will allucinate differently)...\n",
    "\n",
    "Let's try averaging 5 flow models (note that we could do this in parallel with MPI on bigger machines):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "          'feedback': 1,\n",
    "          'verbose': -1,\n",
    "          'plot_every': 1000,\n",
    "          'pop_size': 1,\n",
    "          'num_flows': 5,\n",
    "          'epochs': 400,\n",
    "        }\n",
    "\n",
    "average_flow = sp.average_flow_from_chain(chain,  # parameter difference chain\n",
    "                                                                         **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most methods are implemented for the average flow as well:\n",
    "average_flow.training_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and we can print the training summary, which in this case contains more info:\n",
    "average_flow.print_training_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_samples = average_flow.MCSamples(20000)\n",
    "avg_samples.name_tag = 'Average Flow'\n",
    "temp_samples = [_f.MCSamples(20000) for _f in average_flow.flows]\n",
    "for i, _s in enumerate(temp_samples):\n",
    "    _s.name_tag = _s.name_tag + f'_{i}'\n",
    "# let's plot the flows:\n",
    "g = plots.get_subplot_plotter()\n",
    "g.triangle_plot([chain, avg_samples] + temp_samples,\n",
    "                filled=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logP = average_flow.log_probability(average_flow.cast(avg_samples.samples)).numpy()\n",
    "avg_samples.addDerived(logP, name='logP', label='\\\\log P')\n",
    "avg_samples.updateBaseStatistics();\n",
    "\n",
    "# now let's plot everything:\n",
    "g = plots.get_subplot_plotter()\n",
    "g.triangle_plot([avg_samples, chain], \n",
    "                plot_3d_with_param='logP',\n",
    "                filled=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real world application: joint parameter estimation\n",
    "\n",
    "In this example we perform a flow-based analysis of a joint posterior.\n",
    "\n",
    "The idea is that we have posteriors samples from two independent experiments, we learn the two posteriors and then we combine them to form the joint posterior.\n",
    "\n",
    "Note that we are assuming - as it is true in this example - that the prior is the same among the two experiments and flat (so that we are not duplicating the prior).\n",
    "\n",
    "This procedure was used, for example, in [Gatti, Campailla et al (2024), arXiv:2405.10881](https://arxiv.org/abs/2405.10881).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we start by loading up the posteriors:\n",
    "\n",
    "# load the samples (remove no burn in since the example chains have already been cleaned):\n",
    "chains_dir = os.path.realpath(os.path.join(os.getcwd(), '../..', 'test_chains'))\n",
    "# the Planck 2018 TTTEEE chain:\n",
    "chain_1 = getdist.mcsamples.loadMCSamples(file_root=os.path.join(chains_dir, 'Planck18TTTEEE'), no_cache=True, settings=getdist_settings)\n",
    "# the DES Y1 3x2 chain:\n",
    "chain_2 = getdist.mcsamples.loadMCSamples(file_root=os.path.join(chains_dir, 'DES'), no_cache=True, settings=getdist_settings)\n",
    "# the joint chain:\n",
    "chain_12 = getdist.mcsamples.loadMCSamples(file_root=os.path.join(chains_dir, 'Planck18TTTEEE_DES'), no_cache=True, settings=getdist_settings)\n",
    "\n",
    "# let's add omegab as a derived parameter:\n",
    "for _ch in [chain_1, chain_2, chain_12]:\n",
    "    _p = _ch.getParams()\n",
    "    _h = _p.H0 / 100.\n",
    "    _ch.addDerived(_p.omegabh2 / _h**2, name='omegab', label='\\\\Omega_b')\n",
    "    _ch.updateBaseStatistics()\n",
    "\n",
    "# we define the parameters of the problem:\n",
    "param_names = ['H0', 'omegam', 'sigma8', 'ns', 'omegab']\n",
    "\n",
    "# and then do a sanity check plot:\n",
    "g = plots.get_subplot_plotter()\n",
    "g.triangle_plot([chain_1, chain_2, chain_12], params=param_names, filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we then train the flows on the base parameters that we want to combine (note that for this exercise we should include all shared parameters):\n",
    "kwargs = {\n",
    "          'feedback': 1,\n",
    "          'verbose': -1,\n",
    "          'plot_every': 1000,\n",
    "          'pop_size': 1,\n",
    "          'num_flows': 3,\n",
    "          'epochs': 400,\n",
    "        }\n",
    "\n",
    "# actual flow training:\n",
    "flow_1 = sp.average_flow_from_chain(chain_1, param_names=param_names, **kwargs)\n",
    "flow_2 = sp.average_flow_from_chain(chain_2, param_names=param_names, **kwargs)\n",
    "flow_12 = sp.average_flow_from_chain(chain_12, param_names=param_names, **kwargs)\n",
    "\n",
    "# plot to make sure training went well:\n",
    "flow_1.training_plot()\n",
    "flow_2.training_plot()\n",
    "flow_12.training_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check triangle plot:\n",
    "g = plots.get_subplot_plotter()\n",
    "g.triangle_plot([chain_1, flow_1.MCSamples(20000, settings=getdist_settings),\n",
    "                 chain_2, flow_2.MCSamples(20000, settings=getdist_settings),\n",
    "                 chain_12, flow_12.MCSamples(20000, settings=getdist_settings),\n",
    "                 ], \n",
    "                params=param_names,\n",
    "                filled=False)\n",
    "# we log scale the y axis for the logP plot so that we can appreciate the accuracy of the flow on the tails:\n",
    "for i in range(len(param_names)):\n",
    "    _ax = g.subplots[i, i]\n",
    "    _ax.set_yscale('log')\n",
    "    _ax.set_ylim([1.e-5, 1.0])\n",
    "    _ax.set_ylabel('$\\\\log P$')\n",
    "    _ax.tick_params(axis='y', which='both', labelright='on')\n",
    "    _ax.yaxis.set_label_position(\"right\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can define the joint posterior:\n",
    "def joint_log_posterior(H0, omegam, sigma8, ns, omegab):\n",
    "    params = [H0, omegam, sigma8, ns, omegab]\n",
    "    return [flow_1.log_probability(flow_1.cast([params])).numpy()[0] + flow_2.log_probability(flow_2.cast([params])).numpy()[0]]\n",
    "\n",
    "# and sample it:\n",
    "from cobaya.run import run\n",
    "from getdist.mcsamples import MCSamplesFromCobaya\n",
    "\n",
    "parameters = {}\n",
    "for key in param_names:\n",
    "    parameters[key] = {\"prior\": {\"min\": 1.01*max(flow_1.parameter_ranges[key][0], flow_2.parameter_ranges[key][0]),\n",
    "                                 \"max\": 0.99*min(flow_1.parameter_ranges[key][1], flow_2.parameter_ranges[key][1])},\n",
    "                       \"latex\": flow_1.param_labels[flow_1.param_names.index(key)]}\n",
    "info = {\n",
    "    \"likelihood\": {\"joint_log_posterior\": joint_log_posterior},\n",
    "    \"params\": parameters,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCMC sample:\n",
    "\n",
    "# we need a \\sim good initial proposal and starting point, we get them from one of the flows:\n",
    "flow_1_samples = flow_1.sample(10000)\n",
    "flow_1_logPs = flow_1.log_probability(flow_1_samples).numpy()\n",
    "flow_1_maxP_sample = flow_1_samples[np.argmax(flow_1_logPs)].numpy()\n",
    "\n",
    "# we need a good starting point otherwise this will take long...\n",
    "for _i, _k in enumerate(parameters.keys()):\n",
    "    info['params'][_k]['ref'] = flow_1_maxP_sample[_i]\n",
    "\n",
    "info[\"sampler\"] = {\"mcmc\": \n",
    "                {'covmat': np.cov(flow_1_samples.numpy().T),\n",
    "                 'covmat_params': param_names,\n",
    "                 'max_tries': np.inf,\n",
    "                 'Rminus1_stop': 0.01,\n",
    "                 'learn_proposal_Rminus1_max': 30.,\n",
    "                 'learn_proposal_Rminus1_max_early': 30.,\n",
    "                 'measure_speeds': False,\n",
    "                 'Rminus1_single_split': 10,\n",
    "                 }}\n",
    "info['debug'] = 100  # note this is an insane hack to disable very verbose output...\n",
    "updated_info, sampler = run(info)\n",
    "joint_chain = MCSamplesFromCobaya(updated_info, sampler.products()[\"sample\"], ignore_rows=0.3, settings=getdist_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Nested sampling sample:\n",
    "#_dim = len(flow_1.param_names)\n",
    "#\n",
    "#info[\"sampler\"] = {\"polychord\": {'nlive': 50*_dim,\n",
    "#                                 'measure_speeds': False,\n",
    "#                                 'num_repeats': 2*_dim,\n",
    "#                                 'nprior': 10*25*_dim,\n",
    "#                                 'do_clustering': True,\n",
    "#                                 'precision_criterion': 0.01,\n",
    "#                                 'boost_posterior': 10, \n",
    "#                                 'feedback': 0,\n",
    "#                                 },\n",
    "#                    }\n",
    "#info['debug'] = 100  # note this is an insane hack to disable very verbose output...\n",
    "#updated_info, sampler = run(info)\n",
    "#joint_chain = MCSamplesFromCobaya(updated_info, sampler.products()[\"sample\"], settings=getdist_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_chain.name_tag = 'Flow Joint'\n",
    "chain_12.name_tag = 'Real Joint (Planck + DES)'\n",
    "\n",
    "# sanity check triangle plot:\n",
    "g = plots.get_subplot_plotter()\n",
    "g.triangle_plot([joint_chain, chain_12], \n",
    "                params=param_names,\n",
    "                filled=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see this works fairly well, given that the two experiments are in some tension - do not overlap significantly.\n",
    "\n",
    "Make sure you check for the consistency of the experiments you are combining before doing so, to ensure that the joint flow posterior samples a well-trained part of the flows.\n",
    "\n",
    "You can check the example notebook in this documentation for how to compute tensions between two experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Topic: accurate likelihood values\n",
    "\n",
    "For some applications we need to push the local accuracy of the flow model. In this case we need to provide exact probability values (up to normalization constant) for the training set.\n",
    "\n",
    "These are then used to build a part of the loss function that rewards local accuracy of probability values. This second part of the loss function is the estimated evidence error.\n",
    "By default the code adaptively mixes the two loss functions to find an optimal solution.\n",
    "\n",
    "As a downside we can only train a flow that preserves all the parameters of the distribution, i.e. we cannot train on marginalized parameters (as we have done in the previous examples).\n",
    "\n",
    "For more details see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev, eer = flow.evidence()\n",
    "print(f'log(Z) = {ev} +- {eer}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the value is close to what it should be (zero since the original distribution is normalized) but the estimated error is still fairly high.\n",
    "\n",
    "Since we have (normalized) log P values we can check the local reliability of the normalizing flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_flow_log10_P = flow.log_probability(flow.cast(chain.samples[flow.test_idx, :])).numpy()/np.log(10.)\n",
    "validation_samples_log10_P = -chain.loglikes[flow.test_idx]/np.log(10.)  # notice the minus sign due to the definition of logP in getdist\n",
    "training_flow_log10_P = flow.log_probability(flow.cast(chain.samples[flow.training_idx, :])).numpy()/np.log(10.)\n",
    "training_samples_log10_P = -chain.loglikes[flow.training_idx]/np.log(10.)  # notice the minus sign due to the definition of logP in getdist\n",
    "\n",
    "# do the plot:\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ax1.scatter(training_samples_log10_P - np.amax(training_samples_log10_P), training_flow_log10_P - training_samples_log10_P, s=0.1, label='training')\n",
    "ax1.scatter(validation_samples_log10_P - np.amax(validation_samples_log10_P), validation_flow_log10_P - validation_samples_log10_P, s=0.5, label='validation')\n",
    "ax1.legend()\n",
    "ax1.axhline(0, color='k', linestyle='--')\n",
    "ax1.set_xlabel('$\\log_{10}(P_{\\\\rm true}/P_{\\\\rm max})$')\n",
    "ax1.set_ylabel('$\\log_{10}(P_{\\\\rm flow}) - \\log_{10}(P_{\\\\rm true})$')\n",
    "ax1.set_ylim(-1.0, 1.0)\n",
    "\n",
    "ax2.hist(training_flow_log10_P - training_samples_log10_P, bins=50, range=[-1., 1.], density=True, alpha=0.5, label='training')\n",
    "ax2.hist(validation_flow_log10_P - validation_samples_log10_P, bins=50, range=[-1., 1.], density=True, alpha=0.5, label='validation')\n",
    "ax2.legend()\n",
    "ax2.axvline(0, color='k', linestyle='--')\n",
    "ax2.set_xlabel('$\\log_{10}(P_{\\\\rm flow}) - \\log_{10}(P_{\\\\rm true})$')\n",
    "ax2.set_xlim([-1.0, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the local accuracy of the flow in full dimension is not high. As we move to the tails we easily have large errors. The variance of this plot is the estimated error on the evidence, which is rather large and dominated by the outliers in the tails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering average flows usually improves the situation, in particular on the validation sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev, eer = average_flow.evidence()\n",
    "print(f'log(Z) = {ev} +- {eer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_flow_log10_P = average_flow.log_probability(average_flow.cast(chain.samples[average_flow.test_idx, :])).numpy()/np.log(10.)\n",
    "validation_samples_log10_P = -chain.loglikes[average_flow.test_idx]/np.log(10.)  # notice the minus sign due to the definition of logP in getdist\n",
    "training_flow_log10_P = average_flow.log_probability(average_flow.cast(chain.samples[average_flow.training_idx, :])).numpy()/np.log(10.)\n",
    "training_samples_log10_P = -chain.loglikes[average_flow.training_idx]/np.log(10.)  # notice the minus sign due to the definition of logP in getdist\n",
    "\n",
    "# do the plot:\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ax1.scatter(training_samples_log10_P - np.amax(training_samples_log10_P), training_flow_log10_P - training_samples_log10_P, s=0.1, label='training')\n",
    "ax1.scatter(validation_samples_log10_P - np.amax(validation_samples_log10_P), validation_flow_log10_P - validation_samples_log10_P, s=0.5, label='validation')\n",
    "ax1.legend()\n",
    "ax1.axhline(0, color='k', linestyle='--')\n",
    "ax1.set_xlabel('$\\log_{10}(P_{\\\\rm true}/P_{\\\\rm max})$')\n",
    "ax1.set_ylabel('$\\log_{10}(P_{\\\\rm flow}) - \\log_{10}(P_{\\\\rm true})$')\n",
    "ax1.set_ylim(-1.0, 1.0)\n",
    "\n",
    "ax2.hist(training_flow_log10_P - training_samples_log10_P, bins=50, range=[-1., 1.], density=True, alpha=0.5, label='training')\n",
    "ax2.hist(validation_flow_log10_P - validation_samples_log10_P, bins=50, range=[-1., 1.], density=True, alpha=0.5, label='validation')\n",
    "ax2.legend()\n",
    "ax2.axvline(0, color='k', linestyle='--')\n",
    "ax2.set_xlabel('$\\log_{10}(P_{\\\\rm flow}) - \\log_{10}(P_{\\\\rm true})$')\n",
    "ax2.set_xlim([-1.0, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks significantly better, and in fact the error on the evidence estimate is lower...\n",
    "\n",
    "If we want to do better we need to train with evidence error loss, as discussed in the reference paper for this example notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "          'feedback': 1,\n",
    "          'verbose': -1,\n",
    "          'plot_every': 1000,\n",
    "          'pop_size': 1,\n",
    "          'num_flows': 1,\n",
    "          'epochs': 400,\n",
    "          'loss_mode': 'softadapt',\n",
    "        }\n",
    "\n",
    "average_flow_2 = sp.average_flow_from_chain(chain,  # parameter difference chain\n",
    "                                            **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_flow_2.training_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the training plots are substantially more complicated as we are monitoring several additional quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev, eer = average_flow_2.evidence()\n",
    "print(f'log(Z) = {ev} +- {eer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_flow_log10_P = average_flow_2.log_probability(average_flow_2.cast(chain.samples[average_flow_2.test_idx, :])).numpy()/np.log(10.)\n",
    "validation_samples_log10_P = -chain.loglikes[average_flow_2.test_idx]/np.log(10.)  # notice the minus sign due to the definition of logP in getdist\n",
    "training_flow_log10_P = average_flow_2.log_probability(average_flow_2.cast(chain.samples[average_flow_2.training_idx, :])).numpy()/np.log(10.)\n",
    "training_samples_log10_P = -chain.loglikes[average_flow_2.training_idx]/np.log(10.)  # notice the minus sign due to the definition of logP in getdist\n",
    "\n",
    "# do the plot:\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ax1.scatter(training_samples_log10_P - np.amax(training_samples_log10_P), training_flow_log10_P - training_samples_log10_P, s=0.1, label='training')\n",
    "ax1.scatter(validation_samples_log10_P - np.amax(validation_samples_log10_P), validation_flow_log10_P - validation_samples_log10_P, s=0.5, label='validation')\n",
    "ax1.legend()\n",
    "ax1.axhline(0, color='k', linestyle='--')\n",
    "ax1.set_xlabel('$\\log_{10}(P_{\\\\rm true}/P_{\\\\rm max})$')\n",
    "ax1.set_ylabel('$\\log_{10}(P_{\\\\rm flow}) - \\log_{10}(P_{\\\\rm true})$')\n",
    "ax1.set_ylim(-1.0, 1.0)\n",
    "\n",
    "ax2.hist(training_flow_log10_P - training_samples_log10_P, bins=50, range=[-1., 1.], density=True, alpha=0.5, label='training')\n",
    "ax2.hist(validation_flow_log10_P - validation_samples_log10_P, bins=50, range=[-1., 1.], density=True, alpha=0.5, label='validation')\n",
    "ax2.legend()\n",
    "ax2.axvline(0, color='k', linestyle='--')\n",
    "ax2.set_xlabel('$\\log_{10}(P_{\\\\rm flow}) - \\log_{10}(P_{\\\\rm true})$')\n",
    "ax2.set_xlim([-1.0, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see this achieves performances that are very close to averaging flows. Combining the two strategies achieves the best performances (but is slower to train)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Topic: Spline Flows\n",
    "\n",
    "When more flexibility in the normalizing flow model is needed we provide an implementation of neural spline flows as discussed in [Durkan et al (2019), arXiv:1906.04032](https://arxiv.org/abs/1906.04032)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "          # flow settings:\n",
    "          'pop_size': 1,\n",
    "          'num_flows': 1,\n",
    "          'epochs': 400,\n",
    "          'transformation_type': 'spline',\n",
    "          'autoregressive_type': 'masked',\n",
    "          # feedback flags:\n",
    "          'feedback': 1,\n",
    "          'verbose': -1,\n",
    "          'plot_every': 1000,\n",
    "        }\n",
    "\n",
    "spline_flow = sp.flow_from_chain(chain,  # parameter difference chain\n",
    "                                 **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can plot training summaries to make sure training went smoothly:\n",
    "spline_flow.training_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can triangle plot the flow to see how well it has learned the target distribution:\n",
    "g = plots.get_subplot_plotter()\n",
    "g.triangle_plot([chain, \n",
    "                 spline_flow.MCSamples(20000)  # this flow method returns a MCSamples object\n",
    "                 ], \n",
    "                params=flow.param_names,\n",
    "                filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = spline_flow.MCSamples(20000)\n",
    "logP = spline_flow.log_probability(spline_flow.cast(samples.samples)).numpy()\n",
    "samples.addDerived(logP, name='logP', label='\\\\log P')\n",
    "samples.updateBaseStatistics();\n",
    "\n",
    "# now let's plot everything:\n",
    "g = plots.get_subplot_plotter()\n",
    "g.triangle_plot([samples, chain], \n",
    "                plot_3d_with_param='logP',\n",
    "                filled=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_flow_log10_P = spline_flow.log_probability(spline_flow.cast(chain.samples[spline_flow.test_idx, :])).numpy()/np.log(10.)\n",
    "validation_samples_log10_P = -chain.loglikes[spline_flow.test_idx]/np.log(10.)  # notice the minus sign due to the definition of logP in getdist\n",
    "training_flow_log10_P = spline_flow.log_probability(spline_flow.cast(chain.samples[spline_flow.training_idx, :])).numpy()/np.log(10.)\n",
    "training_samples_log10_P = -chain.loglikes[spline_flow.training_idx]/np.log(10.)  # notice the minus sign due to the definition of logP in getdist\n",
    "\n",
    "ev, eer = spline_flow.evidence()\n",
    "print(f'log(Z) = {ev} +- {eer}')\n",
    "\n",
    "# do the plot:\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ax1.scatter(training_samples_log10_P - np.amax(training_samples_log10_P), training_flow_log10_P - training_samples_log10_P, s=0.1, label='training')\n",
    "ax1.scatter(validation_samples_log10_P - np.amax(validation_samples_log10_P), validation_flow_log10_P - validation_samples_log10_P, s=0.5, label='validation')\n",
    "ax1.legend()\n",
    "ax1.axhline(0, color='k', linestyle='--')\n",
    "ax1.set_xlabel('$\\log_{10}(P_{\\\\rm true}/P_{\\\\rm max})$')\n",
    "ax1.set_ylabel('$\\log_{10}(P_{\\\\rm flow}) - \\log_{10}(P_{\\\\rm true})$')\n",
    "ax1.set_ylim(-1.0, 1.0)\n",
    "\n",
    "ax2.hist(training_flow_log10_P - training_samples_log10_P, bins=50, range=[-1., 1.], density=True, alpha=0.5, label='training')\n",
    "ax2.hist(validation_flow_log10_P - validation_samples_log10_P, bins=50, range=[-1., 1.], density=True, alpha=0.5, label='validation')\n",
    "ax2.legend()\n",
    "ax2.axvline(0, color='k', linestyle='--')\n",
    "ax2.set_xlabel('$\\log_{10}(P_{\\\\rm flow}) - \\log_{10}(P_{\\\\rm true})$')\n",
    "ax2.set_xlim([-1.0, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check what happens across the bijector layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensiometer.synthetic_probability import flow_utilities as flow_utils\n",
    "\n",
    "training_samples_spaces, validation_samples_spaces = \\\n",
    "    flow_utils.get_samples_bijectors(spline_flow, \n",
    "                                     feedback=True)\n",
    "    \n",
    "for i, _s in enumerate(training_samples_spaces):\n",
    "    print('*  ', _s.name_tag)\n",
    "    g = plots.get_subplot_plotter()\n",
    "    g.triangle_plot([\n",
    "                    training_samples_spaces[i],\n",
    "                    validation_samples_spaces[i]], \n",
    "                    filled=True,\n",
    "                    )\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
